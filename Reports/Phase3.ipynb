{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "ccf6f9a1",
      "cell_type": "markdown",
      "source": "# Phase 3 – Integrated Data Mining Notebook  \n## Water Potability Prediction (IT326) /  توقّع صلاحية مياه الشرب\n\nThis notebook presents\n\n1. **Phase 1 – Exploratory Data Analysis (EDA)**  \n2. **Phase 2 – Data Preprocessing**  \n3. **Phase 3 – Classification (Decision Trees) and Clustering (K-means)**  \n\n",
      "metadata": {}
    },
    {
      "id": "6908ba20",
      "cell_type": "markdown",
      "source": "## 1. Environment Setup / إعداد بيئة العمل",
      "metadata": {}
    },
    {
      "id": "4abb84f2",
      "cell_type": "code",
      "source": "# Import required libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.decomposition import PCA\n\nsns.set(style=\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (8, 5)\n\nprint(\"Libraries imported successfully.\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "07c7bd90",
      "cell_type": "markdown",
      "source": "## 2. Problem & Data Overview / وصف المشكلة والبيانات (Phase 1)\n\n- **Problem:** Predict whether a water sample is potable (safe to drink) based on its chemical attributes.  \n- **Dataset:** Water Potability dataset (Kaggle).  \n- **Target attribute:** `Potability` (0 = not potable, 1 = potable).  \n\n> الهدف: بناء نماذج تنقيب بيانات لتوقّع صلاحية الماء للشرب، بالإضافة إلى اكتشاف مجموعات طبيعية (Clusters) داخل البيانات.\n",
      "metadata": {}
    },
    {
      "id": "c0059efc",
      "cell_type": "markdown",
      "source": "### 2.1 Load Raw Dataset / قراءة البيانات الخام",
      "metadata": {}
    },
    {
      "id": "3c64316a",
      "cell_type": "code",
      "source": "# Load RAW dataset from GitHub\n# Update the URL if the path or file name changes in the repository.\n\nurl_raw = \"https://raw.githubusercontent.com/lin-010/IT326-Water-Potability/refs/heads/main/Dataset/Raw_dataset.csv\"\ndf_raw = pd.read_csv(url_raw)\n\nprint(\"Raw dataset shape:\", df_raw.shape)\ndf_raw.head()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f2d2df9d",
      "cell_type": "markdown",
      "source": "## 3. Exploratory Data Analysis (EDA) / تحليل استكشافي (Phase 1)\n\nQuick inspection of:\n\n- Data types  \n- Summary statistics  \n- Class distribution  \n- Missing values  \n\n> في هذا الجزء نكوّن صورة عامة عن البيانات قبل البدء في المعالجة والبناء النماذج.\n",
      "metadata": {}
    },
    {
      "id": "ce5df0c4",
      "cell_type": "code",
      "source": "# Dataset info\ndf_raw.info()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b56bc264",
      "cell_type": "code",
      "source": "# Summary statistics\ndf_raw.describe()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "15361ce6",
      "cell_type": "code",
      "source": "# Class distribution for Potability\nif \"Potability\" in df_raw.columns:\n    print(\"Class distribution (Potability):\")\n    print(df_raw[\"Potability\"].value_counts())\nelse:\n    print(\"Warning: 'Potability' column not found – check dataset.\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "321eac0d",
      "cell_type": "code",
      "source": "# Missing values per column\ndf_raw.isnull().sum()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "30b99261",
      "cell_type": "markdown",
      "source": "## 4. Data Preprocessing / معالجة البيانات (Phase 2)\n\nThe preprocessing pipeline used in this notebook:\n\n1. Work on a copy of the raw dataset.  \n2. Handle missing numeric values using **median imputation**.  \n3. Keep the numeric scale as-is for Decision Trees (no scaling required).  \n4. Later, apply **StandardScaler** separately for K-means (distance-based).  \n",
      "metadata": {}
    },
    {
      "id": "fe6bd645",
      "cell_type": "code",
      "source": "# Start preprocessing from the raw dataset\ndf = df_raw.copy()\n\n# Median imputation for numeric columns\nnumeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\nfor col in numeric_cols:\n    median_value = df[col].median()\n    df[col].fillna(median_value, inplace=True)\n\n# Check missing values after imputation\nprint(\"Missing values after median imputation:\")\nprint(df.isnull().sum())\n\nprint(\"\\nPreprocessed dataset shape:\", df.shape)\ndf.head()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "63d89780",
      "cell_type": "markdown",
      "source": "### 4.1 Features and Target / فصل المتغيرات عن المخرج\n\n- `X` = input features (attributes)  \n- `y` = target class label (`Potability`)",
      "metadata": {}
    },
    {
      "id": "477f84a0",
      "cell_type": "code",
      "source": "# Separate features (X) and target (y)\n\ntarget_column = \"Potability\"\n\nif target_column not in df.columns:\n    raise ValueError(f\"Target column '{target_column}' not found in dataframe.\")\n\nX = df.drop(target_column, axis=1)\ny = df[target_column]\n\nprint(\"Shape of features (X):\", X.shape)\nprint(\"Shape of target (y):\", y.shape)\nprint(\"\\nClass distribution:\")\nprint(y.value_counts())",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "32c21180",
      "cell_type": "markdown",
      "source": "## 5. Decision Tree Classification / التصنيف باستخدام شجرة القرار (Phase 3)\n\nWe build Decision Tree models using:\n\n- **Criteria:** `gini` and `entropy`  \n- **Train/Test splits:** 90/10, 80/20, 70/30  \n\nFor each configuration we compute:\n\n- Accuracy on the test set  \n- Confusion matrix  \n\n",
      "metadata": {}
    },
    {
      "id": "4beb49d1",
      "cell_type": "code",
      "source": "# Helper function to train and evaluate a Decision Tree\n\ndef run_decision_tree(X, y, train_size, criterion, random_state=42):\n    \"\"\"Train a DecisionTreeClassifier and return model, accuracy, confusion matrix, and data splits.\"\"\"\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y,\n        train_size=train_size,\n        random_state=random_state,\n        stratify=y\n    )\n\n    clf = DecisionTreeClassifier(criterion=criterion, random_state=random_state)\n    clf.fit(X_train, y_train)\n\n    y_pred = clf.predict(X_test)\n\n    acc = accuracy_score(y_test, y_pred)\n    cm = confusion_matrix(y_test, y_pred)\n\n    return clf, acc, cm, (X_train, X_test, y_train, y_test, y_pred)\n\n\ntrain_sizes = [0.9, 0.8, 0.7]\ncriteria = [\"gini\", \"entropy\"]\n\nresults_clf = []       # summary table\nmodels_clf = {}        # store models and confusion matrices\n\nfor crit in criteria:\n    for ts in train_sizes:\n        clf, acc, cm, data_split = run_decision_tree(X, y, train_size=ts, criterion=crit)\n\n        results_clf.append({\n            \"criterion\": crit,\n            \"train_size\": ts,\n            \"test_size\": round(1 - ts, 2),\n            \"accuracy\": acc\n        })\n\n        key = f\"{crit}_train{int(ts * 100)}\"\n        models_clf[key] = {\n            \"model\": clf,\n            \"confusion_matrix\": cm,\n            \"data_split\": data_split\n        }\n\nresults_clf_df = pd.DataFrame(results_clf)\nresults_clf_df.sort_values(by=[\"train_size\", \"criterion\"], ascending=[False, True])",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "479612b8",
      "cell_type": "code",
      "source": "# Confusion matrices for all Decision Tree configurations\n\nfor key, info in models_clf.items():\n    cm = info[\"confusion_matrix\"]\n    _, X_test, _, y_test, y_pred = info[\"data_split\"]\n\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n    disp.plot(values_format='d')\n    plt.title(f\"Confusion Matrix - {key.replace('_', ', ')}\")\n    plt.show()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ce223dc1",
      "cell_type": "code",
      "source": "# Best model (highest accuracy) and tree visualization\n\nbest_idx = results_clf_df[\"accuracy\"].idxmax()\nbest_row = results_clf_df.loc[best_idx]\nbest_key = f\"{best_row['criterion']}_train{int(best_row['train_size'] * 100)}\"\n\nprint(\"Best model settings:\")\nprint(best_row)\n\nbest_model = models_clf[best_key][\"model\"]\n\nplt.figure(figsize=(16, 8))\nplot_tree(\n    best_model,\n    feature_names=X.columns,\n    class_names=[\"Not potable (0)\", \"Potable (1)\"],\n    filled=True,\n    rounded=True,\n    fontsize=8\n)\nplt.title(f\"Decision Tree - Best model ({best_key})\")\nplt.show()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a2a25692",
      "cell_type": "markdown",
      "source": "## 6. K-means Clustering / التجميع باستخدام K-means (Phase 3)\n\nWe apply K-means clustering on the feature space (without the label):\n\n1. Drop `Potability` from the feature set.  \n2. Standardize features using `StandardScaler`.  \n3. Try K ∈ {2, 3, 4, 5}.  \n4. For each K, compute:\n   - Total within-cluster sum of squares (**inertia**)  \n   - Average **silhouette score**  \n5. Use Elbow and Silhouette plots to choose the most suitable K.  \n\n",
      "metadata": {}
    },
    {
      "id": "b3a5e3f6",
      "cell_type": "code",
      "source": "# Feature space for K-means (without target)\n\nX_features = df.drop(target_column, axis=1)\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_features)\n\n# K-means for different K values\nk_values = [2, 3, 4, 5]\n\nkmeans_results = []\ncluster_labels_dict = {}\n\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    labels = kmeans.fit_predict(X_scaled)\n\n    inertia = kmeans.inertia_\n    sil_score = silhouette_score(X_scaled, labels)\n\n    kmeans_results.append({\n        \"K\": k,\n        \"inertia\": inertia,\n        \"silhouette\": sil_score\n    })\n\n    cluster_labels_dict[k] = {\n        \"model\": kmeans,\n        \"labels\": labels\n    }\n\nkmeans_results_df = pd.DataFrame(kmeans_results)\nkmeans_results_df.sort_values(\"K\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3b904c28",
      "cell_type": "code",
      "source": "# Elbow plot (K vs inertia)\n\nplt.plot(kmeans_results_df[\"K\"], kmeans_results_df[\"inertia\"], marker=\"o\")\nplt.xlabel(\"Number of clusters (K)\")\nplt.ylabel(\"Total within-cluster sum of squares (Inertia)\")\nplt.title(\"Elbow Method for K-means\")\nplt.xticks(k_values)\nplt.show()\n\n# Silhouette plot (K vs silhouette)\n\nplt.plot(kmeans_results_df[\"K\"], kmeans_results_df[\"silhouette\"], marker=\"o\")\nplt.xlabel(\"Number of clusters (K)\")\nplt.ylabel(\"Average Silhouette Coefficient\")\nplt.title(\"Silhouette Scores for Different K\")\nplt.xticks(k_values)\nplt.show()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "dfddff55",
      "cell_type": "code",
      "source": "# Best K based on silhouette score and PCA visualization\n\nbest_k_idx = kmeans_results_df[\"silhouette\"].idxmax()\nbest_k = int(kmeans_results_df.loc[best_k_idx, \"K\"])\nprint(\"Best K based on silhouette:\", best_k)\n\nbest_labels = cluster_labels_dict[best_k][\"labels\"]\n\n# PCA to 2D\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=best_labels, alpha=0.6)\nplt.xlabel(\"PCA Component 1\")\nplt.ylabel(\"PCA Component 2\")\nplt.title(f\"K-means Clusters (K = {best_k}) in PCA space\")\nplt.show()\n\n# Attach cluster labels to original data and summarize\n\ndf_clusters = df.copy()\ndf_clusters[\"cluster\"] = best_labels\n\ncluster_summary = df_clusters.groupby(\"cluster\").mean(numeric_only=True)\ncluster_summary",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "362fd77c",
      "cell_type": "markdown",
      "source": "## 7. Summary of Results / ملخّص النتائج\n\nThis section summarizes the main outputs obtained from the analysis:\n\n- `results_clf_df`: جدول يوضّح دقة نماذج شجرة القرار مع اختلاف معيار التقسيم ونسب تقسيم البيانات إلى تدريب/اختبار.  \n- Confusion matrices: توضح أداء النموذج في التمييز بين العينات الصالحة للشرب (1) وغير الصالحة (0).  \n- Best Decision Tree plot: يوضّح أهم المتغيرات المؤثرة في التنبؤ بصلاحية الماء وبعض قواعد التصنيف المستخلصة من الشجرة.  \n- `kmeans_results_df`: جدول يحتوي قيم الـ inertia والـ silhouette لمجموعة من قيم K في خوارزمية K-means.  \n- Elbow & Silhouette plots: تساعد في اختيار قيمة K الأنسب بناءً على شكل المنحنيات.  \n- `cluster_summary` مع مخطط الـ PCA: يبيّن خصائص كل Cluster من حيث متوسط قيم المتغيرات وربطها بجودة الماء.\n\n",
      "metadata": {}
    }
  ]
}