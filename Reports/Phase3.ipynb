{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8c555a5",
   "metadata": {},
   "source": [
    "# Phase 3 – Integrated Data Mining Notebook  \n",
    "## Water Potability Prediction (IT326)\n",
    "\n",
    "This notebook combines the work of **Phase 1 (EDA)**, **Phase 2 (Preprocessing)**, and **Phase 3 (Modeling)** in a single clean pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### Structure\n",
    "\n",
    "1. **Problem & Data Overview (Phase 1)**\n",
    "2. **Exploratory Data Analysis (Phase 1)**\n",
    "3. **Preprocessing Pipeline (Phase 2)**\n",
    "4. **Decision Tree Classification (Phase 3 – Classification)**\n",
    "5. **K-means Clustering (Phase 3 – Clustering)**\n",
    "6. **Summary of Results (for PDF & Presentation)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c235fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3 – Setup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26dd826",
   "metadata": {},
   "source": [
    "## 1. Problem & Data Overview (Phase 1)\n",
    "\n",
    "- **Goal:** Predict whether a water sample is **potable (safe to drink)** based on its chemical characteristics.  \n",
    "- **Task type:** Supervised classification (for prediction) + unsupervised clustering (to discover natural groups in the data).  \n",
    "- **Dataset:** Water Potability dataset (Kaggle).  \n",
    "- **Target attribute:** `Potability` (0 = not potable, 1 = potable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411f4aba",
   "metadata": {},
   "source": [
    "### 1.1 Load Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RAW dataset from GitHub\n",
    "# Update the URL if the file name/path changes in the repository.\n",
    "\n",
    "url_raw = \"https://raw.githubusercontent.com/lin-010/IT326-Water-Potability/refs/heads/main/Dataset/Raw_dataset.csv\"\n",
    "df_raw = pd.read_csv(url_raw)\n",
    "\n",
    "print(\"Raw dataset shape:\", df_raw.shape)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00f3844",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (Phase 1)\n",
    "\n",
    "Quick inspection of:\n",
    "\n",
    "- Data types  \n",
    "- Summary statistics  \n",
    "- Class distribution  \n",
    "- Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bedc768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data info\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d4f896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numeric columns\n",
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d698616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution for Potability (target)\n",
    "if \"Potability\" in df_raw.columns:\n",
    "    print(\"Class distribution (Potability):\")\n",
    "    print(df_raw[\"Potability\"].value_counts())\n",
    "else:\n",
    "    print(\"Warning: 'Potability' column not found – check dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1236b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values per column\n",
    "df_raw.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdefcf",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Pipeline (Phase 2)\n",
    "\n",
    "We apply a single preprocessing pipeline that will be reused in both classification and clustering:\n",
    "\n",
    "1. Work on a copy of the raw data.  \n",
    "2. Handle missing numeric values using **median imputation**.  \n",
    "3. Keep the numeric scale as-is for Decision Trees (they do not require scaling).  \n",
    "4. Later, apply **StandardScaler** separately for K-means only (because it is distance-based)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88177378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start preprocessing from the raw dataset\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Median imputation for numeric columns\n",
    "numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "for col in numeric_cols:\n",
    "    median_value = df[col].median()\n",
    "    df[col].fillna(median_value, inplace=True)\n",
    "\n",
    "# Check missing values after imputation\n",
    "print(\"Missing values after median imputation:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nPreprocessed dataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028fcaaf",
   "metadata": {},
   "source": [
    "### 3.1 Features and Target\n",
    "\n",
    "We now separate:\n",
    "\n",
    "- `X` = all predictor attributes  \n",
    "- `y` = class label `Potability`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4616776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "\n",
    "target_column = \"Potability\"\n",
    "\n",
    "if target_column not in df.columns:\n",
    "    raise ValueError(f\"Target column '{target_column}' not found in dataframe.\")\n",
    "\n",
    "X = df.drop(target_column, axis=1)\n",
    "y = df[target_column]\n",
    "\n",
    "print(\"Shape of features (X):\", X.shape)\n",
    "print(\"Shape of target (y):\", y.shape)\n",
    "print(\"\\nClass distribution:\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070eecb6",
   "metadata": {},
   "source": [
    "## 4. Decision Tree Classification (Phase 3 – Classification)\n",
    "\n",
    "We train Decision Tree models using:\n",
    "\n",
    "- Splitting criteria: **Gini** and **Entropy**  \n",
    "- Train/test splits: **90/10**, **80/20**, **70/30**  \n",
    "\n",
    "For each configuration we compute:\n",
    "\n",
    "- Accuracy on the test set  \n",
    "- Confusion matrix  \n",
    "\n",
    "We then identify the best model and visualize its tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f32b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for training and evaluating a Decision Tree\n",
    "\n",
    "def run_decision_tree(X, y, train_size, criterion, random_state=42):\n",
    "    \"\"\"Train a DecisionTreeClassifier and return model, accuracy, confusion matrix, and data splits.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        train_size=train_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    clf = DecisionTreeClassifier(criterion=criterion, random_state=random_state)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return clf, acc, cm, (X_train, X_test, y_train, y_test, y_pred)\n",
    "\n",
    "\n",
    "train_sizes = [0.9, 0.8, 0.7]\n",
    "criteria = [\"gini\", \"entropy\"]\n",
    "\n",
    "results_clf = []       # summary table\n",
    "models_clf = {}        # store models and confusion matrices\n",
    "\n",
    "for crit in criteria:\n",
    "    for ts in train_sizes:\n",
    "        clf, acc, cm, data_split = run_decision_tree(X, y, train_size=ts, criterion=crit)\n",
    "\n",
    "        results_clf.append({\n",
    "            \"criterion\": crit,\n",
    "            \"train_size\": ts,\n",
    "            \"test_size\": round(1 - ts, 2),\n",
    "            \"accuracy\": acc\n",
    "        })\n",
    "\n",
    "        key = f\"{crit}_train{int(ts * 100)}\"\n",
    "        models_clf[key] = {\n",
    "            \"model\": clf,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"data_split\": data_split\n",
    "        }\n",
    "\n",
    "results_clf_df = pd.DataFrame(results_clf)\n",
    "results_clf_df.sort_values(by=[\"train_size\", \"criterion\"], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d6611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for all configurations\n",
    "\n",
    "for key, info in models_clf.items():\n",
    "    cm = info[\"confusion_matrix\"]\n",
    "    _, X_test, _, y_test, y_pred = info[\"data_split\"]\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "    disp.plot(values_format='d')\n",
    "    plt.title(f\"Confusion Matrix - {key.replace('_', ', ')}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b38d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model (highest accuracy) and tree visualization\n",
    "\n",
    "best_idx = results_clf_df[\"accuracy\"].idxmax()\n",
    "best_row = results_clf_df.loc[best_idx]\n",
    "best_key = f\"{best_row['criterion']}_train{int(best_row['train_size'] * 100)}\"\n",
    "\n",
    "print(\"Best model settings:\")\n",
    "print(best_row)\n",
    "\n",
    "best_model = models_clf[best_key][\"model\"]\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plot_tree(\n",
    "    best_model,\n",
    "    feature_names=X.columns,\n",
    "    class_names=[\"Not potable (0)\", \"Potable (1)\"],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=8\n",
    ")\n",
    "plt.title(f\"Decision Tree - Best model ({best_key})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5254096",
   "metadata": {},
   "source": [
    "## 5. K-means Clustering (Phase 3 – Clustering)\n",
    "\n",
    "We perform unsupervised clustering using **K-means** on the feature space (without the label):\n",
    "\n",
    "1. Drop the `Potability` column.  \n",
    "2. Standardize features using `StandardScaler`.  \n",
    "3. Try multiple values of **K** (2, 3, 4, 5).  \n",
    "4. For each K, compute:\n",
    "   - Total within-cluster sum of squares (**inertia**)  \n",
    "   - Average **silhouette score**  \n",
    "5. Use:\n",
    "   - **Elbow method** (K vs inertia)  \n",
    "   - **Silhouette scores** (K vs silhouette)  \n",
    "6. Visualize clusters in 2D using **PCA**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824fe8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature space for K-means (without target)\n",
    "\n",
    "X_features = df.drop(target_column, axis=1)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_features)\n",
    "\n",
    "# K-means for different K values\n",
    "k_values = [2, 3, 4, 5]\n",
    "\n",
    "kmeans_results = []\n",
    "cluster_labels_dict = {}\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    inertia = kmeans.inertia_\n",
    "    sil_score = silhouette_score(X_scaled, labels)\n",
    "\n",
    "    kmeans_results.append({\n",
    "        \"K\": k,\n",
    "        \"inertia\": inertia,\n",
    "        \"silhouette\": sil_score\n",
    "    })\n",
    "\n",
    "    cluster_labels_dict[k] = {\n",
    "        \"model\": kmeans,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "kmeans_results_df = pd.DataFrame(kmeans_results)\n",
    "kmeans_results_df.sort_values(\"K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d944df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow plot (K vs inertia)\n",
    "\n",
    "plt.plot(kmeans_results_df[\"K\"], kmeans_results_df[\"inertia\"], marker=\"o\")\n",
    "plt.xlabel(\"Number of clusters (K)\")\n",
    "plt.ylabel(\"Total within-cluster sum of squares (Inertia)\")\n",
    "plt.title(\"Elbow Method for K-means\")\n",
    "plt.xticks(k_values)\n",
    "plt.show()\n",
    "\n",
    "# Silhouette plot (K vs silhouette)\n",
    "\n",
    "plt.plot(kmeans_results_df[\"K\"], kmeans_results_df[\"silhouette\"], marker=\"o\")\n",
    "plt.xlabel(\"Number of clusters (K)\")\n",
    "plt.ylabel(\"Average Silhouette Coefficient\")\n",
    "plt.title(\"Silhouette Scores for Different K\")\n",
    "plt.xticks(k_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de360368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best K based on silhouette score and PCA visualization\n",
    "\n",
    "best_k_idx = kmeans_results_df[\"silhouette\"].idxmax()\n",
    "best_k = int(kmeans_results_df.loc[best_k_idx, \"K\"])\n",
    "print(\"Best K based on silhouette:\", best_k)\n",
    "\n",
    "best_labels = cluster_labels_dict[best_k][\"labels\"]\n",
    "\n",
    "# PCA to 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=best_labels, alpha=0.6)\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.title(f\"K-means Clusters (K = {best_k}) in PCA space\")\n",
    "plt.show()\n",
    "\n",
    "# Attach cluster labels to original data and summarize\n",
    "\n",
    "df_clusters = df.copy()\n",
    "df_clusters[\"cluster\"] = best_labels\n",
    "\n",
    "cluster_summary = df_clusters.groupby(\"cluster\").mean(numeric_only=True)\n",
    "cluster_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcd0bef",
   "metadata": {},
   "source": [
    "## 6. Summary of Results (For PDF Report & Presentation)\n",
    "\n",
    "From this notebook you will use the following in the **final PDF report** and **presentation**:\n",
    "\n",
    "- `results_clf_df`: Table of Decision Tree configurations (criterion, train/test split, accuracy).  \n",
    "- Confusion matrix plots: To comment on which class is harder to predict.  \n",
    "- Best Decision Tree visualization: To highlight important features and example rules.  \n",
    "- `kmeans_results_df`: Table of K vs inertia vs silhouette to justify best K.  \n",
    "- Elbow and silhouette plots: Visual justification for chosen K.  \n",
    "- PCA cluster plot and `cluster_summary`: To describe cluster profiles and relate them to water quality.\n",
    "\n",
    "In the PDF report you will:\n",
    "\n",
    "- Explain **why** you used each method and from which **package** (e.g., `DecisionTreeClassifier` and `KMeans` from `sklearn`).  \n",
    "- Compare your results (even roughly) with the **research paper** results.  \n",
    "- Provide a clear **discussion and conclusion** based on these outputs.\n",
    "\n",
    "This notebook is the **technical backbone** for Phase 3 and will be linked in the GitHub submission.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
