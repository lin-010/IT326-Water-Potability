{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d30a000",
   "metadata": {},
   "source": [
    "# Phase 3 – Final Report: Water Potability (IT326)\n",
    "\n",
    "This notebook combines the work of **Phase 1, Phase 2, and Phase 3** in a single, clean file:\n",
    "\n",
    "- Introduces the **problem**, **data mining task**, and **dataset**.\n",
    "- Shows the main **data preprocessing** pipeline (once, without duplicated code).\n",
    "- Applies **classification** (Decision Trees).\n",
    "- Applies **clustering** (K-Means) as required.\n",
    "- Summarizes **evaluation, comparison, and findings**.\n",
    "\n",
    "> **Note:** You can adjust the text (problem description, references, etc.) to match your report and research paper exactly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffc745b",
   "metadata": {},
   "source": [
    "## [1] Problem\n",
    "\n",
    "Access to **safe drinking water** is a critical global challenge. Contaminated water can cause serious health problems such as gastrointestinal diseases, neurological disorders, and other long-term complications.\n",
    "\n",
    "In this project, we work with a water quality dataset to decide whether a given water sample is **potable (safe to drink)** or **not potable** based on measured chemical and physical attributes. Our goal is to support decision-makers and water treatment facilities by:\n",
    "\n",
    "- **Predicting** if a water sample is potable or not.\n",
    "- **Discovering patterns / groups** of water samples using clustering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7f40fd",
   "metadata": {},
   "source": [
    "## [2] Data Mining Task\n",
    "\n",
    "We consider two main data mining tasks:\n",
    "\n",
    "1. **Classification**  \n",
    "   - Task: Predict whether a water sample is **potable** (`1`) or **not potable** (`0`).  \n",
    "   - Technique: **Decision Tree classifier** using two splitting criteria:\n",
    "     - Gini index\n",
    "     - Information Gain (entropy)\n",
    "\n",
    "2. **Clustering**  \n",
    "   - Task: Group water samples into clusters to understand hidden structure in the data.  \n",
    "   - Technique: **K-Means** algorithm with several values of K, using:\n",
    "     - **Elbow method** (based on within-cluster sum of squares / inertia)\n",
    "     - **Silhouette coefficient** for cluster quality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff08224",
   "metadata": {},
   "source": [
    "## [3] Data\n",
    "\n",
    "The dataset contains measurements for multiple water samples. Each row represents **one sample**, and each column is a measured attribute.\n",
    "\n",
    "Typical attributes include (depending on the original dataset version):\n",
    "\n",
    "- **pH** – acidity/basicity of the water.\n",
    "- **Hardness** – amount of dissolved calcium and magnesium.\n",
    "- **Solids** – total dissolved solids.\n",
    "- **Chloramines**\n",
    "- **Sulfate**\n",
    "- **Conductivity**\n",
    "- **Organic carbon**\n",
    "- **Trihalomethanes**\n",
    "- **Turbidity**\n",
    "- **Potability** – **target label** (1 = potable, 0 = not potable).\n",
    "\n",
    "In the next cell, we load the dataset from the project repository and briefly inspect its structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739de92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    silhouette_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 5)\n",
    "\n",
    "# === Load raw dataset from GitHub ===\n",
    "url = \"https://raw.githubusercontent.com/lin-010/IT326-Water-Potability/refs/heads/main/Dataset/Raw_dataset.csv\"\n",
    "df_raw = pd.read_csv(url)\n",
    "\n",
    "print(\"Raw dataset shape:\", df_raw.shape)\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d73e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick info and missing values\n",
    "\n",
    "print(\"\\nDataset info:\")\n",
    "df_raw.info()\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df_raw.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistical summary\n",
    "df_raw.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c259d8",
   "metadata": {},
   "source": [
    "## [4] Data Preprocessing\n",
    "\n",
    "In this section, we prepare the data for both **classification** and **clustering**.\n",
    "\n",
    "Main steps:\n",
    "\n",
    "1. **Handle missing values**  \n",
    "   - For numeric attributes, we replace missing values with the **median** of each column.\n",
    "   - This is a simple and robust strategy that reduces the impact of outliers.\n",
    "\n",
    "2. **Ensure correct data types**  \n",
    "   - The target column `Potability` is kept as an integer/binary label.\n",
    "\n",
    "3. **Create two versions of the dataset**  \n",
    "   - `df_clean`: cleaned dataset (after handling missing values).  \n",
    "   - `X_clust`: feature matrix (only input attributes, without the target) to be used for clustering.  \n",
    "   - For clustering, we will also apply **standardization** using `StandardScaler`.\n",
    "\n",
    "> If you already used a different preprocessing strategy in Phase 2 (e.g., other imputation, removing outliers), you can replace the code below with your exact steps. The important point is that the preprocessing is written **once** in this notebook and then reused for both classification and clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c220e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the raw dataset for preprocessing\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Name of the target column (adjust if your column name is different)\n",
    "target_col = \"Potability\"\n",
    "\n",
    "# Check that the target column exists\n",
    "if target_col not in df.columns:\n",
    "    raise ValueError(f\"Target column '{target_col}' not found. Please update 'target_col' to match your dataset.\")\n",
    "\n",
    "# Handle missing values: fill numeric columns with their median\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "for col in numeric_cols:\n",
    "    median_value = df[col].median()\n",
    "    df[col].fillna(median_value, inplace=True)\n",
    "\n",
    "# (Optional) If there are non-numeric columns you decided to drop or encode,\n",
    "# you can handle them here. For this dataset we expect mostly numeric columns.\n",
    "\n",
    "# Ensure target is integer (0/1)\n",
    "df[target_col] = df[target_col].astype(int)\n",
    "\n",
    "print(\"Cleaned dataset shape:\", df.shape)\n",
    "print(\"Any remaining missing values?\", df.isnull().sum().any())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e531e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y) for classification\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "print(\"\\nClass distribution:\")\n",
    "print(y.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feafa5d8",
   "metadata": {},
   "source": [
    "## [5] Data Mining Technique\n",
    "\n",
    "### 5.1 Classification – Decision Trees\n",
    "\n",
    "For the classification task, we use the **DecisionTreeClassifier** from the `sklearn.tree` package.\n",
    "\n",
    "We compare two attribute selection measures:\n",
    "\n",
    "- **Gini index** (default in scikit-learn)\n",
    "- **Information Gain (entropy)**\n",
    "\n",
    "For each criterion, we train and evaluate models using three train-test partitions:\n",
    "\n",
    "- 90% train – 10% test\n",
    "- 80% train – 20% test\n",
    "- 70% train – 30% test\n",
    "\n",
    "For each model we compute:\n",
    "\n",
    "- Accuracy\n",
    "- Confusion matrix\n",
    "\n",
    "At the end, we identify the **best model** and visualize its decision tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb5b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate Decision Trees with different criteria and train-test splits\n",
    "\n",
    "splits = [\n",
    "    (0.9, 0.1),\n",
    "    (0.8, 0.2),\n",
    "    (0.7, 0.3),\n",
    "]\n",
    "\n",
    "criteria = [\"gini\", \"entropy\"]\n",
    "\n",
    "results_clf = []\n",
    "\n",
    "for train_size, test_size in splits:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        train_size=train_size,\n",
    "        stratify=y,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    for crit in criteria:\n",
    "        clf = DecisionTreeClassifier(\n",
    "            criterion=crit,\n",
    "            random_state=42\n",
    "        )\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        results_clf.append({\n",
    "            \"criterion\": crit,\n",
    "            \"train_size\": f\"{int(train_size*100)}%\",\n",
    "            \"test_size\": f\"{int(test_size*100)}%\",\n",
    "            \"accuracy\": acc,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"model\": clf,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_test\": y_test\n",
    "        })\n",
    "\n",
    "# Summary table with accuracies\n",
    "summary_rows = [\n",
    "    {\n",
    "        \"Criterion\": r[\"criterion\"],\n",
    "        \"Train %\": r[\"train_size\"],\n",
    "        \"Test %\": r[\"test_size\"],\n",
    "        \"Accuracy\": round(r[\"accuracy\"], 4)\n",
    "    }\n",
    "    for r in results_clf\n",
    "]\n",
    "\n",
    "clf_results_df = pd.DataFrame(summary_rows)\n",
    "clf_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d767c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for each trained model\n",
    "\n",
    "for r in results_clf:\n",
    "    cm = r[\"confusion_matrix\"]\n",
    "    clf = r[\"model\"]\n",
    "    X_test = r[\"X_test\"]\n",
    "    y_test = r[\"y_test\"]\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=cm,\n",
    "        display_labels=clf.classes_\n",
    "    )\n",
    "    disp.plot()\n",
    "    plt.title(\n",
    "        f\"Confusion Matrix – criterion={r['criterion']}, \"\n",
    "        f\"train={r['train_size']}, test={r['test_size']}\"\n",
    "    )\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f889c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and visualize the best performing model\n",
    "\n",
    "best_model = max(results_clf, key=lambda r: r[\"accuracy\"])\n",
    "best_clf = best_model[\"model\"]\n",
    "\n",
    "print(\"Best model parameters:\")\n",
    "print(\"Criterion:\", best_model[\"criterion\"])\n",
    "print(\"Train size:\", best_model[\"train_size\"])\n",
    "print(\"Test size:\", best_model[\"test_size\"])\n",
    "print(\"Accuracy:\", round(best_model[\"accuracy\"], 4))\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plot_tree(\n",
    "    best_clf,\n",
    "    feature_names=X.columns,\n",
    "    class_names=[\"Not potable\", \"Potable\"],\n",
    "    filled=True,\n",
    "    fontsize=8\n",
    ")\n",
    "plt.title(\"Decision Tree – Best Model\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865db9a6",
   "metadata": {},
   "source": [
    "### 5.2 Clustering – K-Means\n",
    "\n",
    "For the clustering task, we use the **K-Means** algorithm from the `sklearn.cluster` package.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Use all **feature columns** (without the target label) as input.\n",
    "2. Apply **standardization** using `StandardScaler` so that all attributes have similar scale.\n",
    "3. Try several values of **K** (number of clusters), for example: 2, 3, 4, 5.\n",
    "4. For each K, compute:\n",
    "   - **Total within-cluster sum of squares (inertia)** – used in the **Elbow method**.\n",
    "   - **Average silhouette coefficient** – measures cluster quality.\n",
    "5. Choose the best K based on silhouette and Elbow plot.\n",
    "6. Visualize the clusters in 2D using two selected features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09cba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for clustering\n",
    "\n",
    "# X_clust = feature matrix for clustering (without the target label)\n",
    "X_clust = df.drop(columns=[target_col])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_clust)\n",
    "\n",
    "print(\"Shape of X_scaled:\", X_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f1166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try multiple values of K\n",
    "\n",
    "ks = [2, 3, 4, 5]\n",
    "silhouette_scores = []\n",
    "inertias = []\n",
    "kmodels = {}\n",
    "\n",
    "for k in ks:\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=k,\n",
    "        n_init=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    inertia = kmeans.inertia_\n",
    "    inertias.append(inertia)\n",
    "\n",
    "    sil_score = silhouette_score(X_scaled, labels)\n",
    "    silhouette_scores.append(sil_score)\n",
    "\n",
    "    kmodels[k] = {\n",
    "        \"model\": kmeans,\n",
    "        \"labels\": labels,\n",
    "        \"silhouette\": sil_score,\n",
    "        \"inertia\": inertia\n",
    "    }\n",
    "\n",
    "# Summary table for clustering results\n",
    "clust_summary = pd.DataFrame({\n",
    "    \"K\": ks,\n",
    "    \"Average Silhouette\": np.round(silhouette_scores, 4),\n",
    "    \"Total Within-Cluster SS (Inertia)\": np.round(inertias, 2)\n",
    "})\n",
    "clust_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e44f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow plot (Inertia vs K)\n",
    "\n",
    "plt.plot(ks, inertias, marker=\"o\")\n",
    "plt.xlabel(\"Number of clusters K\")\n",
    "plt.ylabel(\"Total within-cluster sum of squares (Inertia)\")\n",
    "plt.title(\"Elbow Method for K-Means\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ae2b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette coefficient vs K\n",
    "\n",
    "plt.plot(ks, silhouette_scores, marker=\"o\")\n",
    "plt.xlabel(\"Number of clusters K\")\n",
    "plt.ylabel(\"Average Silhouette Coefficient\")\n",
    "plt.title(\"Silhouette Coefficient for different K\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eb7f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose best K based on silhouette score\n",
    "\n",
    "best_k_index = int(np.argmax(silhouette_scores))\n",
    "best_k = ks[best_k_index]\n",
    "\n",
    "print(\"Best K based on silhouette score:\", best_k)\n",
    "print(\"\\nClustering summary:\")\n",
    "print(clust_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452d4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters in 2D using two selected features\n",
    "\n",
    "feature_names = X_clust.columns\n",
    "feat_x = feature_names[0]\n",
    "feat_y = feature_names[1]\n",
    "\n",
    "for k in ks:\n",
    "    labels = kmodels[k][\"labels\"]\n",
    "    \n",
    "    plt.scatter(\n",
    "        X_clust[feat_x],\n",
    "        X_clust[feat_y],\n",
    "        c=labels,\n",
    "        s=10\n",
    "    )\n",
    "    plt.xlabel(feat_x)\n",
    "    plt.ylabel(feat_y)\n",
    "    plt.title(f\"K-Means Clusters (K={k}) using {feat_x} vs {feat_y}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc12df3",
   "metadata": {},
   "source": [
    "## [6] Evaluation and Comparison\n",
    "\n",
    "### 6.1 Classification Results and Discussion\n",
    "\n",
    "Using Decision Trees, we compared:\n",
    "\n",
    "- **Criteria**: Gini vs Entropy.\n",
    "- **Train–Test splits**: 90–10, 80–20, 70–30.\n",
    "\n",
    "From the results table:\n",
    "\n",
    "- We can identify which **criterion** gives higher accuracy overall.\n",
    "- We can check if a more balanced split (e.g., 80–20 or 70–30) leads to more stable performance compared to 90–10.\n",
    "\n",
    "When discussing the results in the PDF report, it is important to:\n",
    "\n",
    "- Highlight the **best model** (criterion, split, accuracy).\n",
    "- Comment on the **confusion matrices** (e.g., more false positives vs false negatives).\n",
    "- Compare the obtained accuracy with the results reported in the **research paper** (even if it is only an approximate comparison).\n",
    "\n",
    "### 6.2 Clustering Results and Discussion\n",
    "\n",
    "From the clustering summary:\n",
    "\n",
    "- We compare the **Average Silhouette** scores across different K values.\n",
    "- We inspect the **Elbow plot** to see where the inertia decreases start to slow down.\n",
    "- We select the **best K** according to the silhouette and Elbow method.\n",
    "\n",
    "In the discussion, we can mention:\n",
    "\n",
    "- Why we chose a specific K as the final number of clusters.\n",
    "- How the clusters differ in terms of some attributes (for example, one cluster may correspond to samples with higher solids or different pH values).\n",
    "- Any relation between the clusters and the `Potability` label (even if the label is not used during clustering).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd21bec",
   "metadata": {},
   "source": [
    "## [7] Findings and Discussion\n",
    "\n",
    "In this section you summarize the main insights from both classification and clustering. For example:\n",
    "\n",
    "- **Classification**\n",
    "  - Decision Trees achieved an accuracy of around *X%* using the best configuration.\n",
    "  - The most important attributes (according to the tree structure) may include features such as **pH**, **Solids**, **Sulfate**, etc.\n",
    "  - Compared to the research paper, our results are (similar / slightly lower / higher), possibly due to differences in preprocessing, parameter settings, or dataset size.\n",
    "\n",
    "- **Clustering**\n",
    "  - K-Means discovered *K* meaningful clusters in the data.\n",
    "  - Clusters show different profiles of water quality attributes (e.g., one cluster with lower turbidity and higher pH).\n",
    "  - Clustering can help understand natural groupings of water samples and may support decision-making about which samples require treatment.\n",
    "\n",
    "Finally, you can state which technique (classification vs clustering) is more useful for the **end user** (e.g., a water treatment engineer):\n",
    "- Classification is useful for **predicting potability** of new samples.\n",
    "- Clustering is useful for **exploratory analysis** and understanding structure in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7228e2fd",
   "metadata": {},
   "source": [
    "## [8] References\n",
    "\n",
    "Add your references here in the required format (e.g., IEEE). Typical references:\n",
    "\n",
    "1. The original **water potability dataset** (Kaggle link).\n",
    "2. The **research paper** you used for comparison.\n",
    "3. Any additional articles or documentation you used for the methods (e.g., scikit-learn documentation).\n",
    "\n",
    "Make sure the same references are used consistently in both the **notebook**, the **PDF final report**, and the **presentation**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
