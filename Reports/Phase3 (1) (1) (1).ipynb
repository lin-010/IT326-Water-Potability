{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "c9c0e748",
      "cell_type": "markdown",
      "source": "# Phase3 – Final Report: Water Potability Dataset",
      "metadata": {}
    },
    {
      "id": "bfdf2282",
      "cell_type": "markdown",
      "source": "## [1] Problem\n\nAccess to clean and safe drinking water is essential for human health. However, water sources may contain chemical and physical contaminants that make the water unsafe for human consumption.\n\nIn this project, we work with a *water potability* dataset to determine whether a given water sample is **potable (safe to drink)** or **not potable** based on several measured attributes (such as pH, hardness, solids, sulfate, and others). Our main goal is to support decision-making about water quality by applying data mining techniques to:\n- Predict if a water sample is potable.\n- Discover natural groups (clusters) of water samples.\n",
      "metadata": {}
    },
    {
      "id": "dace2511",
      "cell_type": "markdown",
      "source": "This project aims to apply data mining techniques to the Water Potability dataset to predict whether water samples are safe for human consumption (Potability = 1) or not (Potability = 0).",
      "metadata": {}
    },
    {
      "id": "b8866adc",
      "cell_type": "markdown",
      "source": "## [2] Data Mining Task\n\nWe formalize the problem as two main data mining tasks:\n\n1. **Classification task**  \n   - Goal: predict the class label `Potability` for each water sample, where:\n     - `0` = Not potable (not safe to drink)\n     - `1` = Potable (safe to drink)\n   - Technique: **Decision Tree classifier** using two attribute selection measures:\n     - Gini index (default in scikit-learn)\n     - Information Gain (entropy)\n\n2. **Clustering task**  \n   - Goal: group water samples into clusters based on their attribute values to discover hidden structure.\n   - Technique: **K-Means clustering** with several values of K. We evaluate K using:\n     - **Silhouette coefficient**\n     - **Elbow method** (total within-cluster sum of squares / inertia)\n\nThese tasks will be applied to the **preprocessed dataset** prepared in Phase 2.\n",
      "metadata": {}
    },
    {
      "id": "607e209f",
      "cell_type": "markdown",
      "source": "## [3] Data\n\nThe dataset used in this project is the **Water Potability dataset** from Kaggle, created by *Aditya Kadiwal*. It contains **9 numeric features** describing the chemical and physical characteristics of water samples, plus a target attribute **Potability** that indicates whether the sample is safe for human consumption.\n\n### 3.1 Dataset Overview\n- **Source:** Kaggle — [Water Potability Dataset](https://www.kaggle.com/datasets/adityakadiwal/water-potability)\n- **File used:** `Dataset/Raw_dataset.csv`\n- **Target column:** `Potability` (0 = Not drinkable, 1 = Drinkable)\n- **Number of features:** 9 numeric attributes (pH, Hardness, Solids, Chloramines, Sulfate, Conductivity, Organic_carbon, Trihalomethanes, Turbidity)\n\n### 3.2 Observations from Phase 1\n- Dataset includes some **missing values** in columns such as `ph`, `Sulfate`, and `Trihalomethanes`.\n- Several features show **outliers** and **skewed distributions**.\n- Class distribution is **imbalanced** (fewer potable samples).\n- All features are **numeric**, so no encoding is required.\n\nThese findings helped guide the preprocessing steps in Phase 2. The code below loads the dataset, displays its structure, and summarizes the key information (shape, column names, dtypes, and basic statistics).\n",
      "metadata": {}
    },
    {
      "id": "35e5240d",
      "cell_type": "markdown",
      "source": "## Dataset Source\n- **Dataset:** Water Potability (by Aditya Kadiwal)  \n- **Platform:** Kaggle  \n- **Canonical link:** https://www.kaggle.com/datasets/adityakadiwal/water-potability  \n- **Repository file used:** `Dataset/Raw_dataset.csv` (tracked in our GitHub repo)\n\n### Data Access & Integrity\n- We load the CSV directly from the repository to ensure the notebook is **self‑contained**.  \n- Integrity checks performed in code right after this cell:\n  - Presence of the **`Potability`** column (hard failure if missing).\n  - Display of **shape**, **dtypes**, **head(10)**, and **describe()** for transparency.\n  - **Missing‑value audit** to quantify NaNs before preprocessing.\n\n### Versioning Note\n- The file path is fixed in the repo; any future updates to the raw file should keep the same schema (same column headers) to preserve reproducibility for Phase 3 experiments.\n",
      "metadata": {}
    },
    {
      "id": "d9e27c23",
      "cell_type": "code",
      "source": "import pandas as pd\n\nurl = \"https://raw.githubusercontent.com/lin-010/IT326-Water-Potability/main/Dataset/Raw_dataset.csv\"\ndata = pd.read_csv(url)\n\nprint(\"Dataset loaded successfully!\")\nprint(\"Shape (rows, cols):\", data.shape)\n\n# quick check that Potability column exists\nif 'Potability' not in data.columns:\n    raise ValueError(\"The dataset does not contain a 'Potability' column. Check the CSV headers.\")\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cd48c74e",
      "cell_type": "code",
      "source": "# Sample rows and dataset basics\nfrom IPython.display import display\n\nprint(\"Shape (rows, cols):\", data.shape)\nprint(\"\\nFirst 10 rows:\")\ndisplay(data.head(10))\n\nprint(\"\\nColumn names:\")\nprint(list(data.columns))\n\nprint(\"\\nData types:\")\ndisplay(data.dtypes)\n\nprint(\"\\nStatistical summary (numeric columns):\")\ndisplay(data.describe().T)\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5e3e2614",
      "cell_type": "code",
      "source": "# Number of attributes and feature list\nnum_rows, num_cols = data.shape\nfeature_columns = [c for c in data.columns if c != 'Potability']\nnum_features = len(feature_columns)\n\nprint(f\"Number of instances (rows): {num_rows}\")\nprint(f\"Number of attributes (columns): {num_cols}  -> features: {num_features}, class label: 1 (Potability)\")\nprint(\"\\nFeature names:\")\nprint(feature_columns)\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f49f5f2e",
      "cell_type": "code",
      "source": "# Class distribution\nclass_counts = data['Potability'].value_counts().sort_index()\nclass_percent = data['Potability'].value_counts(normalize=True).sort_index() * 100\n\nprint(\"Class counts (Potability):\")\nprint(class_counts)\nprint(\"\\nClass percentages:\")\nprint(class_percent.round(2).astype(str) + \" %\")\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "31cb8e65",
      "cell_type": "code",
      "source": "# Missing value analysis\nmissing = data.isnull().sum()\nprint(\"Missing values per column:\")\nprint(missing)\nprint(\"\\nTotal missing values in dataset:\", missing.sum())\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "13899855",
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "de1c998d",
      "cell_type": "markdown",
      "source": "## [4] Data Preprocessing\n\nIn Phase 2, the raw dataset was cleaned and normalized to prepare it for data mining. The following preprocessing steps were applied:\n\n1. **Handling missing values:** Missing numeric entries were filled with the **median** value of each column to reduce skew effects.\n2. **Handling outliers:** Applied **IQR-based capping (winsorization)** to limit extreme values without removing rows.\n3. **Normalization:** Scaled all numeric features to the **[0,1] range** using Min–Max normalization.\n4. **Correlation analysis:** Generated a heatmap to check relationships between features and the target variable.\n5. **Saved the clean dataset** as `Preprocessed_dataset.csv` for use in classification and clustering.\n\nThese steps ensure consistent scaling and reduced noise, allowing fair comparison across algorithms. The code below is the same one used in Phase 2 and shows both the raw and preprocessed snapshots.\n",
      "metadata": {}
    },
    {
      "id": "d9eebad4",
      "cell_type": "code",
      "source": "# Water Potability Dataset - Data Analysis\n\n\n# 1. Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nsns.set(style=\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10,6)\n\n\n# 2. Load Dataset\n\nurl = \"https://raw.githubusercontent.com/lin-010/IT326-Water-Potability/refs/heads/main/Dataset/Raw_dataset.csv\"\ndf = pd.read_csv(url)   \n\n\nprint(df.head())\n\n\n# 3. Check Missing Values\n\nprint(\"\\nMissing Values per Column:\\n\")\nprint(df.isnull().sum())\n\n\nplt.figure(figsize=(8,5))\nsns.heatmap(df.isnull(), cbar=False, cmap=\"viridis\")\nplt.title(\"Missing Values Heatmap\")\nplt.show()\n\n\n# 4. Statistical Summary (Five-number summary)\n\nprint(\"\\nStatistical Summary:\\n\")\nprint(df.describe())\n\n\n# 5. Plot 1: Histogram - Variable Distributions\n\ndf.hist(figsize=(12,10), bins=20, color='skyblue')\nplt.suptitle(\"Histograms of Numeric Attributes\", fontsize=16)\nplt.show()\n\n\n# 6. Plot 2: Boxplot - Outliers Detection\n\nplt.figure(figsize=(12,6))\nsns.boxplot(data=df, orient=\"h\")\nplt.title(\"Boxplot for Detecting Outliers\")\nplt.show()\n\n\n# 7. Plot 3: Countplot - Class Label Distribution\n\nplt.figure(figsize=(6,4))\nsns.countplot(x='Potability', data=df, palette='pastel')\nplt.title(\"Class Label Distribution (Potability)\")\nplt.xlabel(\"Potability (0 = Not Drinkable, 1 = Drinkable)\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n# 8. Plot 4: Scatter Plot - Relationship Example\n\nplt.figure(figsize=(7,5))\nsns.scatterplot(x='ph', y='Hardness', hue='Potability', data=df, alpha=0.7)\nplt.title(\"Scatter Plot of pH vs Hardness by Potability\")\nplt.show()\n\n# -----------------------------------------------------------\n# 9. Brief Observations :\n\n# - Missing values exist in several columns, so preprocessing (imputation) is required.\n# - Histograms show that some features like 'ph' and 'Sulfate' are skewed.\n# - Boxplots reveal outliers, especially in 'Sulfate' and 'Turbidity'.\n# - Class label distribution is imbalanced (more non-drinkable samples).\n# - Scatter plot shows weak correlation between pH and Hardness.\n\n# Water Potability Dataset - Data Preprocessing\n# 10. Data Preprocessing\n\n# 10.1 Handle Missing Values (Imputation)\n\n\ndf_filled = df.fillna(df.median())\nprint(\"\\n Missing values handled using median imputation.\\n\")\nprint(df_filled.isnull().sum())\n\n\n# 10.2 Handle Outliers \nQ1 = df_filled.quantile(0.25)\nQ3 = df_filled.quantile(0.75)\nIQR = Q3 - Q1\n\n# Define bounds\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Cap outliers \ndf_no_outliers = df_filled.clip(lower=lower_bound, upper=upper_bound, axis=1)\n\nprint(\"\\n Outliers handled using IQR method.\\n\")\n\n\n# 10.3 Normalization \nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(df_no_outliers.drop('Potability', axis=1))\n\ndf_scaled = pd.DataFrame(scaled_data, columns=df_no_outliers.columns[:-1])\ndf_scaled['Potability'] = df_no_outliers['Potability']\n\nprint(\"\\n Data normalized using Min-Max scaling.\\n\")\nprint(df_scaled.head())\n\n\n# 10.4 Feature Selection (Correlation Analysis)\nplt.figure(figsize=(10,6))\ncorr_matrix = df_scaled.corr()\nsns.heatmap(corr_matrix, annot=False, cmap=\"coolwarm\")\nplt.title(\"Correlation Matrix Heatmap\")\nplt.show()\n\n# Drop low-correlation features \nlow_corr_features = corr_matrix['Potability'][abs(corr_matrix['Potability']) < 0.05].index\nprint(\"Low correlation features (optional to drop):\", list(low_corr_features))\n\n\n# 10.5 Save Preprocessed Data\ndf_scaled.to_csv(\"Preprocessed_dataset.csv\", index=False)\nprint(\"\\n Preprocessed dataset saved as 'Preprocessed_dataset.csv'\\n\")\n\n\n# -----------------------------------------------------------\n# 11. Summary of Preprocessing Steps\n\nprint(\"\"\"Preprocessing Summary:1. Missing values handled with median imputation.2. Outliers capped using IQR-based winsorization.3. Data normalized using Min-Max Scaling (0–1 range).4. Features evaluated using correlation analysis.5. Final dataset saved for Phase 3 (classification & clustering).\"\"\")\n\n\n# Data snapshots:\n\n\nprint(\"Snapshot of Raw Dataset (before preprocessing):\")\nprint(df.head())\n\n\nprint(\"\\nSnapshot of Preprocessed Dataset (after preprocessing):\")\nprint(df_scaled.head())\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d1357df5",
      "cell_type": "markdown",
      "source": "## [5] Data Mining Technique\n\nIn this section, we apply the data mining techniques to the **preprocessed dataset** (`df_scaled` from Section [4]).\n\n### 5.1 Classification – Decision Trees\n\nWe use the `DecisionTreeClassifier` from the `sklearn.tree` package with two different splitting criteria:\n\n- `criterion='gini'`  → Gini index (default)\n- `criterion='entropy'`  → Information Gain (entropy)\n\nFor each criterion, we evaluate three different train–test splits:\n\n- 90% training – 10% testing\n- 80% training – 20% testing\n- 70% training – 30% testing\n\nFor every configuration, we compute the **accuracy** and **confusion matrix**, and then identify the best-performing model.\n",
      "metadata": {}
    },
    {
      "id": "ad71bb25",
      "cell_type": "code",
      "source": "#classsfication dicision tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport pandas as pd\n\n# Features and target\nX = df_scaled.drop(columns=[\"Potability\"])\ny = df_scaled[\"Potability\"]\n\n# Splits and criteria\nsplits = [(0.9, 0.1), (0.8, 0.2), (0.7, 0.3)]\ncriteria = [\"gini\", \"entropy\"]\n\nresults = []\n\nfor train_size, test_size in splits:\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y,\n        train_size=train_size,\n        stratify=y,\n        random_state=42\n    )\n\n    for crit in criteria:\n        clf = DecisionTreeClassifier(criterion=crit, random_state=42)\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n\n        acc = accuracy_score(y_test, y_pred)\n        cm = confusion_matrix(y_test, y_pred)\n\n        results.append({\n            \"Criterion\": crit,\n            \"Train %\": f\"{int(train_size*100)}%\",\n            \"Test %\": f\"{int(test_size*100)}%\",\n            \"Accuracy\": round(acc, 4),\n            \"Confusion Matrix\": cm\n        })\n\n# converting result to table\nresults_df = pd.DataFrame(results)\ndisplay(results_df)\n\n# Best Model\nbest_row = results_df.loc[results_df[\"Accuracy\"].idxmax()]\nprint(\"\\nBest Model:\")\nprint(best_row)\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ed49b44b",
      "cell_type": "markdown",
      "source": "### 5.2 Clustering – K-Means\n\nFor clustering, we use the `KMeans` algorithm from the `sklearn.cluster` package. We apply clustering on the **feature space only** (without the `Potability` label) and standardize the features before applying K-Means.\n\nWe evaluate several values of K (number of clusters), for example:\n- K = 2\n- K = 3\n- K = 4\n- K = 5\n\nFor each K, we compute:\n- **Average Silhouette coefficient**\n- **Total within-cluster sum of squares (inertia)**\n\nThese metrics are then used to select the best K according to the majority rule (considering both silhouette and elbow method). We also visualize the clusters in 2D using two selected features.\n",
      "metadata": {}
    },
    {
      "id": "b05ad3d2",
      "cell_type": "code",
      "source": "# === Clustering using K-Means ===\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Use only feature space (drop target)\nX_clust = df_scaled.drop(columns=[\"Potability\"])\n\n# Standardization\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_clust)\n\n# Values of K to test\nKs = [2, 3, 4, 5]\n\nsilhouette_scores = []\ninertias = []\nmodels = {}\n\nfor k in Ks:\n    kmeans = KMeans(\n        n_clusters=k,\n        n_init=10,\n        random_state=42\n    )\n\n    labels = kmeans.fit_predict(X_scaled)\n\n    inertia = kmeans.inertia_\n    sil_score = silhouette_score(X_scaled, labels)\n\n    inertias.append(inertia)\n    silhouette_scores.append(sil_score)\n\n    models[k] = {\n        \"model\": kmeans,\n        \"labels\": labels,\n        \"silhouette\": sil_score,\n        \"inertia\": inertia\n    }\n\n# Summary Table\nsummary = pd.DataFrame({\n    \"K\": Ks,\n    \"Average Silhouette\": np.round(silhouette_scores, 4),\n    \"Inertia\": np.round(inertias, 2)\n})\n\ndisplay(summary)\n\n# Elbow Plot\nplt.figure(figsize=(6,4))\nplt.plot(Ks, inertias, marker=\"o\")\nplt.title(\"Elbow Method – K-Means\")\nplt.xlabel(\"K\")\nplt.ylabel(\"Inertia\")\nplt.grid(True)\nplt.show()\n\n# Silhouette Plot\nplt.figure(figsize=(6,4))\nplt.plot(Ks, silhouette_scores, marker=\"o\")\nplt.title(\"Silhouette Score – K-Means\")\nplt.xlabel(\"K\")\nplt.ylabel(\"Average Silhouette\")\nplt.grid(True)\nplt.show()\n\n# Choose best K based on highest silhouette\nbest_k = Ks[np.argmax(silhouette_scores)]\nprint(\"Best K based on silhouette score:\", best_k)\n\n# 2D Visualization using first two features\nfeat_x = X_clust.columns[0]\nfeat_y = X_clust.columns[1]\n\nfor k in Ks:\n    labels = models[k][\"labels\"]\n    plt.figure(figsize=(6,4))\n    plt.scatter(\n        X_clust[feat_x],\n        X_clust[feat_y],\n        c=labels,\n        s=15\n    )\n    plt.xlabel(feat_x)\n    plt.ylabel(feat_y)\n    plt.title(f\"K-Means Clusters (K={k}) – 2D Visualization\")\n    plt.show()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4a1bfd05",
      "cell_type": "markdown",
      "source": "## [6] Evaluation and Comparison\n\nIn this section, we summarize and compare the results of the data mining techniques.\n\n### 6.1 Classification\n\nThe table `clf_results_df` (printed above) summarizes the accuracy for each combination of:\n- Splitting criterion (Gini vs Entropy)\n- Train–test partition (90–10, 80–20, 70–30)\n\nFrom this table, we can identify:\n- The best-performing **criterion**.\n- The best **train–test split**.\n- The overall best model (also visualized by the final Decision Tree).\n\n### 6.2 Clustering\n\nThe table `clust_summary` contains, for each K:\n- Average silhouette coefficient\n- Total within-cluster sum of squares (inertia)\n\nUsing the silhouette and elbow plots, we can select the best K and interpret how the clusters separate water samples with different attribute values.\n",
      "metadata": {}
    },
    {
      "id": "3e4a334f",
      "cell_type": "code",
      "source": "# Re-display the main evaluation tables (optional)\n\nprint(\"=== Classification Accuracy Summary ===\")\ntry:\n    display(clf_results_df)\nexcept NameError:\n    print(\"clf_results_df is not defined. Please run the classification cell in Section 5.1.\")\n\nprint(\"\\n=== Clustering Summary ===\")\ntry:\n    display(clust_summary)\nexcept NameError:\n    print(\"clust_summary is not defined. Please run the clustering cell in Section 5.2.\")\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8d22d314",
      "cell_type": "markdown",
      "source": "## [7] Findings and Discussion\n\nIn this section, you should discuss the most important findings from both classification and clustering. For example:\n\n- **Classification**\n  - Which combination of criterion and train–test split achieved the best accuracy?\n  - Are there signs of overfitting (very high accuracy on a small test split)?\n  - Which attributes appear to be most important according to the Decision Tree structure?\n  - How do your results compare (even roughly) with the results reported in the selected research paper that used a similar dataset or problem?\n\n- **Clustering**\n  - What is the best K according to the silhouette and elbow methods?\n  - How can you describe the clusters in terms of the water quality attributes.\n  - Do some clusters tend to contain more potable or more non-potable samples (even though the label was not used during training)?\n\nFinally, summarize which technique (classification or clustering) is more useful for the stakeholders (e.g., water quality engineers) and what practical insights your project provides about water potability.\n",
      "metadata": {}
    },
    {
      "id": "542a6513",
      "cell_type": "markdown",
      "source": "## [8] References\n\nBelow is an example list of references in IEEE style. You should update it to match the exact research paper(s) you used in your project.\n\n1. A. Kadiwal, \"Water Potability,\" *Kaggle Datasets*, 2020. [Online]. Available: <https://www.kaggle.com/datasets/adityakadiwal/water-potability>.\n\n2. **[Replace with your main research paper]** Author(s), \"Title of the paper,\" *Conference or Journal Name*, vol. X, no. Y, pp. Z–AA, Year.\n\n3. F. Pedregosa *et al.*, \"Scikit-learn: Machine Learning in Python,\" *Journal of Machine Learning Research*, vol. 12, pp. 2825–2830, 2011.\n",
      "metadata": {}
    }
  ]
}