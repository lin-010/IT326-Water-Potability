{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "c9c0e748",
      "cell_type": "markdown",
      "source": "# Phase3 – Final Report: Water Potability Dataset",
      "metadata": {}
    },
    {
      "id": "bfdf2282",
      "cell_type": "markdown",
      "source": "## [1] Problem\n\nAccess to clean and safe drinking water is essential for human health. However, water sources may contain chemical and physical contaminants that make the water unsafe for human consumption.\n\nIn this project, we work with a *water potability* dataset to determine whether a given water sample is **potable (safe to drink)** or **not potable** based on several measured attributes (such as pH, hardness, solids, sulfate, and others). Our main goal is to support decision-making about water quality by applying data mining techniques to:\n- Predict if a water sample is potable.\n- Discover natural groups (clusters) of water samples.\n\n",
      "metadata": {}
    },
    {
      "id": "dace2511",
      "cell_type": "markdown",
      "source": "This project aims to apply data mining techniques to the Water Potability dataset to predict whether water samples are safe for human consumption (Potability = 1) or not (Potability = 0).",
      "metadata": {}
    },
    {
      "id": "b8866adc",
      "cell_type": "markdown",
      "source": "## [2] Data Mining Task\n\nWe formalize the problem as two main data mining tasks:\n\n1. **Classification task**  \n   - Goal: predict the class label `Potability` for each water sample, where:\n     - `0` = Not potable (not safe to drink)\n     - `1` = Potable (safe to drink)\n   - Technique: **Decision Tree classifier** using two attribute selection measures:\n     - Gini index (default in scikit-learn)\n     - Information Gain (entropy)\n\n2. **Clustering task**  \n   - Goal: group water samples into clusters based on their attribute values to discover hidden structure.\n   - Technique: **K-Means clustering** with several values of K. We evaluate K using:\n     - **Silhouette coefficient**\n     - **Elbow method** (total within-cluster sum of squares / inertia)\n\nThese tasks will be applied to the **preprocessed dataset** prepared in Phase 2.",
      "metadata": {}
    },
    {
      "id": "607e209f",
      "cell_type": "markdown",
      "source": "## [3] Data\n\nIn this section, we summarize the dataset information and basic exploration steps carried out in **Phase 1**.\n\n### 3.1 Dataset Source and Description\n",
      "metadata": {}
    },
    {
      "id": "35e5240d",
      "cell_type": "markdown",
      "source": "## Dataset Source\n- Dataset: Water Potability (Aditya Kadiwal)  \n- Source: Kaggle  \n- Kaggle link: https://www.kaggle.com/datasets/adityakadiwal/water-potability  \n- Required local filename in repo: `Dataset/Raw_dataset.csv`\n",
      "metadata": {}
    },
    {
      "id": "d9e27c23",
      "cell_type": "code",
      "source": "\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/lin-010/IT326-Water-Potability/main/Dataset/Raw_dataset.csv\"\ndata = pd.read_csv(url)\n\nprint(\"Dataset loaded successfully!\")\nprint(\"Shape (rows, cols):\", data.shape)\n\n# quick check that Potability column exists\nif 'Potability' not in data.columns:\n    raise ValueError(\"The dataset does not contain a 'Potability' column. Check the CSV headers.\")\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cd48c74e",
      "cell_type": "code",
      "source": "# Sample rows and dataset basics\nfrom IPython.display import display\n\nprint(\"Shape (rows, cols):\", data.shape)\nprint(\"\\nFirst 10 rows:\")\ndisplay(data.head(10))\n\nprint(\"\\nColumn names:\")\nprint(list(data.columns))\n\nprint(\"\\nData types:\")\ndisplay(data.dtypes)\n\nprint(\"\\nStatistical summary (numeric columns):\")\ndisplay(data.describe().T)\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5e3e2614",
      "cell_type": "code",
      "source": "# Number of attributes and feature list\nnum_rows, num_cols = data.shape\nfeature_columns = [c for c in data.columns if c != 'Potability']\nnum_features = len(feature_columns)\n\nprint(f\"Number of instances (rows): {num_rows}\")\nprint(f\"Number of attributes (columns): {num_cols}  -> features: {num_features}, class label: 1 (Potability)\")\nprint(\"\\nFeature names:\")\nprint(feature_columns)\n\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f49f5f2e",
      "cell_type": "code",
      "source": "# Class distribution\nclass_counts = data['Potability'].value_counts().sort_index()\nclass_percent = data['Potability'].value_counts(normalize=True).sort_index() * 100\n\nprint(\"Class counts (Potability):\")\nprint(class_counts)\nprint(\"\\nClass percentages:\")\nprint(class_percent.round(2).astype(str) + \" %\")\n\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "31cb8e65",
      "cell_type": "code",
      "source": "# Missing value analysis\nmissing = data.isnull().sum()\nprint(\"Missing values per column:\")\nprint(missing)\nprint(\"\\nTotal missing values in dataset:\", missing.sum())\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "13899855",
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "de1c998d",
      "cell_type": "markdown",
      "source": "## [4] Data Preprocessing\n\nIn **Phase 2**, we analyzed and preprocessed the raw dataset to prepare it for data mining. The main preprocessing steps applied include:\n\n1. **Handling missing values** using median imputation for numeric attributes.\n2. **Handling outliers** using IQR-based winsorization (capping extreme values).\n3. **Normalization** of features using Min-Max scaling to the range [0, 1].\n4. **Feature evaluation** using correlation analysis with a heatmap.\n5. **Saving the final preprocessed dataset** as `Preprocessed_dataset.csv`.\n\nThe following code cell is the original Phase 2 notebook code, which performs both data analysis (plots, summaries) and preprocessing in one place. It defines the final preprocessed dataframe `df_scaled` that will be used in the next sections for classification and clustering.",
      "metadata": {}
    },
    {
      "id": "d9eebad4",
      "cell_type": "code",
      "source": "\n# Water Potability Dataset - Data Analysis\n\n\n# 1. Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nsns.set(style=\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10,6)\n\n\n# 2. Load Dataset\n\nurl = \"https://raw.githubusercontent.com/lin-010/IT326-Water-Potability/refs/heads/main/Dataset/Raw_dataset.csv\"\ndf = pd.read_csv(url)   \n\n\nprint(df.head())\n\n\n# 3. Check Missing Values\n\nprint(\"\\nMissing Values per Column:\\n\")\nprint(df.isnull().sum())\n\n\nplt.figure(figsize=(8,5))\nsns.heatmap(df.isnull(), cbar=False, cmap=\"viridis\")\nplt.title(\"Missing Values Heatmap\")\nplt.show()\n\n\n# 4. Statistical Summary (Five-number summary)\n\nprint(\"\\nStatistical Summary:\\n\")\nprint(df.describe())\n\n\n# 5. Plot 1: Histogram - Variable Distributions\n\ndf.hist(figsize=(12,10), bins=20, color='skyblue')\nplt.suptitle(\"Histograms of Numeric Attributes\", fontsize=16)\nplt.show()\n\n\n# 6. Plot 2: Boxplot - Outliers Detection\n\nplt.figure(figsize=(12,6))\nsns.boxplot(data=df, orient=\"h\")\nplt.title(\"Boxplot for Detecting Outliers\")\nplt.show()\n\n\n# 7. Plot 3: Countplot - Class Label Distribution\n\nplt.figure(figsize=(6,4))\nsns.countplot(x='Potability', data=df, palette='pastel')\nplt.title(\"Class Label Distribution (Potability)\")\nplt.xlabel(\"Potability (0 = Not Drinkable, 1 = Drinkable)\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n# 8. Plot 4: Scatter Plot - Relationship Example\n\nplt.figure(figsize=(7,5))\nsns.scatterplot(x='ph', y='Hardness', hue='Potability', data=df, alpha=0.7)\nplt.title(\"Scatter Plot of pH vs Hardness by Potability\")\nplt.show()\n\n# -----------------------------------------------------------\n# 9. Brief Observations :\n\n# - Missing values exist in several columns, so preprocessing (imputation) is required.\n# - Histograms show that some features like 'ph' and 'Sulfate' are skewed.\n# - Boxplots reveal outliers, especially in 'Sulfate' and 'Turbidity'.\n# - Class label distribution is imbalanced (more non-drinkable samples).\n# - Scatter plot shows weak correlation between pH and Hardness.\n\n# Water Potability Dataset - Data Preprocessing\n# 10. Data Preprocessing\n\n# 10.1 Handle Missing Values (Imputation)\n\n\ndf_filled = df.fillna(df.median())\nprint(\"\\n Missing values handled using median imputation.\\n\")\nprint(df_filled.isnull().sum())\n\n\n# 10.2 Handle Outliers \nQ1 = df_filled.quantile(0.25)\nQ3 = df_filled.quantile(0.75)\nIQR = Q3 - Q1\n\n# Define bounds\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Cap outliers \ndf_no_outliers = df_filled.clip(lower=lower_bound, upper=upper_bound, axis=1)\n\nprint(\"\\n Outliers handled using IQR method.\\n\")\n\n\n# 10.3 Normalization \nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(df_no_outliers.drop('Potability', axis=1))\n\ndf_scaled = pd.DataFrame(scaled_data, columns=df_no_outliers.columns[:-1])\ndf_scaled['Potability'] = df_no_outliers['Potability']\n\nprint(\"\\n Data normalized using Min-Max scaling.\\n\")\nprint(df_scaled.head())\n\n\n# 10.4 Feature Selection (Correlation Analysis)\nplt.figure(figsize=(10,6))\ncorr_matrix = df_scaled.corr()\nsns.heatmap(corr_matrix, annot=False, cmap=\"coolwarm\")\nplt.title(\"Correlation Matrix Heatmap\")\nplt.show()\n\n# Drop low-correlation features \nlow_corr_features = corr_matrix['Potability'][abs(corr_matrix['Potability']) < 0.05].index\nprint(\"Low correlation features (optional to drop):\", list(low_corr_features))\n\n\n# 10.5 Save Preprocessed Data\ndf_scaled.to_csv(\"Preprocessed_dataset.csv\", index=False)\nprint(\"\\n Preprocessed dataset saved as 'Preprocessed_dataset.csv'\\n\")\n\n\n# -----------------------------------------------------------\n# 11. Summary of Preprocessing Steps\n\nprint(\"\"\"\nPreprocessing Summary:\n1. Missing values handled with median imputation.\n2. Outliers capped using IQR-based winsorization.\n3. Data normalized using Min-Max Scaling (0–1 range).\n4. Features evaluated using correlation analysis.\n5. Final dataset saved for Phase 3 (classification & clustering).\n\"\"\")\n\n# Data snapshots:\n\n\nprint(\"Snapshot of Raw Dataset (before preprocessing):\")\nprint(df.head())\n\n\nprint(\"\\nSnapshot of Preprocessed Dataset (after preprocessing):\")\nprint(df_scaled.head())\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d1357df5",
      "cell_type": "markdown",
      "source": "## [5] Data Mining Technique\n\nIn this section, we apply the data mining techniques to the **preprocessed dataset** (`df_scaled` from Section [4]).\n\n### 5.1 Classification – Decision Trees\n\nWe use the `DecisionTreeClassifier` from the `sklearn.tree` package with two different splitting criteria:\n\n- `criterion='gini'`  → Gini index (default)\n- `criterion='entropy'`  → Information Gain (entropy)\n\nFor each criterion, we evaluate three different train–test splits:\n\n- 90% training – 10% testing\n- 80% training – 20% testing\n- 70% training – 30% testing\n\nFor every configuration, we compute the **accuracy** and **confusion matrix**, and then identify the best-performing model.",
      "metadata": {}
    },
    {
      "id": "ad71bb25",
      "cell_type": "code",
      "source": "# === Classification using Decision Trees ===\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n\n# Ensure df_scaled exists (from preprocessing section)\ntry:\n    df_scaled\nexcept NameError:\n    raise NameError(\"df_scaled is not defined. Please run the preprocessing cell in Section [4] first.\")\n\ntarget_col = \"Potability\"\nif target_col not in df_scaled.columns:\n    raise ValueError(f\"Target column '{target_col}' not found in df_scaled.\")\n\n# Features and target\nX = df_scaled.drop(columns=[target_col])\ny = df_scaled[target_col]\n\nprint(\"Preprocessed features shape:\", X.shape)\nprint(\"Target shape:\", y.shape)\nprint(\"\\nClass distribution in preprocessed data:\")\nprint(y.value_counts())\n\n# Train-test splits and criteria\nsplits = [\n    (0.9, 0.1),\n    (0.8, 0.2),\n    (0.7, 0.3),\n]\ncriteria = [\"gini\", \"entropy\"]\n\nresults_clf = []\n\nfor train_size, test_size in splits:\n    X_train, X_test, y_train, y_test = train_test_split(\n        X,\n        y,\n        train_size=train_size,\n        stratify=y,\n        random_state=42\n    )\n    \n    for crit in criteria:\n        clf = DecisionTreeClassifier(\n            criterion=crit,\n            random_state=42\n        )\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n\n        acc = accuracy_score(y_test, y_pred)\n        cm = confusion_matrix(y_test, y_pred)\n\n        results_clf.append({\n            \"criterion\": crit,\n            \"train_size\": f\"{int(train_size*100)}%\",\n            \"test_size\": f\"{int(test_size*100)}%\",\n            \"accuracy\": acc,\n            \"confusion_matrix\": cm,\n            \"model\": clf,\n            \"X_test\": X_test,\n            \"y_test\": y_test\n        })\n\n# Build summary table of accuracies\nsummary_rows = [\n    {\n        \"Criterion\": r[\"criterion\"],\n        \"Train %\": r[\"train_size\"],\n        \"Test %\": r[\"test_size\"],\n        \"Accuracy\": round(r[\"accuracy\"], 4)\n    }\n    for r in results_clf\n]\n\nclf_results_df = pd.DataFrame(summary_rows)\nprint(\"\\nDecision Tree classification results:\")\ndisplay(clf_results_df)\n\n# Plot confusion matrices\nfor r in results_clf:\n    cm = r[\"confusion_matrix\"]\n    clf = r[\"model\"]\n    disp = ConfusionMatrixDisplay(\n        confusion_matrix=cm,\n        display_labels=clf.classes_\n    )\n    disp.plot()\n    plt.title(\n        f\"Confusion Matrix – criterion={r['criterion']}, \"\n        f\"train={r['train_size']}, test={r['test_size']}\"\n    )\n    plt.show()\n\n# Find best model by accuracy\nbest_model = max(results_clf, key=lambda r: r[\"accuracy\"])\nbest_clf = best_model[\"model\"]\n\nprint(\"\\nBest Decision Tree model:\")\nprint(\"  Criterion:\", best_model[\"criterion\"])\nprint(\"  Train size:\", best_model[\"train_size\"])\nprint(\"  Test size:\", best_model[\"test_size\"])\nprint(\"  Accuracy:\", round(best_model[\"accuracy\"], 4))\n\n# Visualize the best tree\nplt.figure(figsize=(16, 8))\nplot_tree(\n    best_clf,\n    feature_names=X.columns,\n    class_names=[\"Not potable\", \"Potable\"],\n    filled=True,\n    fontsize=8\n)\nplt.title(\"Decision Tree – Best Model\")\nplt.show()\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ed49b44b",
      "cell_type": "markdown",
      "source": "### 5.2 Clustering – K-Means\n\nFor clustering, we use the `KMeans` algorithm from the `sklearn.cluster` package. We apply clustering on the **feature space only** (without the `Potability` label) and standardize the features before applying K-Means.\n\nWe evaluate several values of K (number of clusters), for example:\n- K = 2\n- K = 3\n- K = 4\n- K = 5\n\nFor each K, we compute:\n- **Average Silhouette coefficient**\n- **Total within-cluster sum of squares (inertia)**\n\nThese metrics are then used to select the best K according to the majority rule (considering both silhouette and elbow method). We also visualize the clusters in 2D using two selected features.",
      "metadata": {}
    },
    {
      "id": "b05ad3d2",
      "cell_type": "code",
      "source": "# === Clustering using K-Means ===\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Use feature columns (drop target)\nX_clust = df_scaled.drop(columns=[target_col])\n\n# Standardize features for K-Means\nscaler = StandardScaler()\nX_scaled_clust = scaler.fit_transform(X_clust)\n\nprint(\"Clustering feature matrix shape:\", X_scaled_clust.shape)\n\nks = [2, 3, 4, 5]\nsilhouette_scores = []\ninertias = []\nkmodels = {}\n\nfor k in ks:\n    kmeans = KMeans(\n        n_clusters=k,\n        n_init=10,\n        random_state=42\n    )\n    labels = kmeans.fit_predict(X_scaled_clust)\n\n    inertia = kmeans.inertia_\n    inertias.append(inertia)\n\n    sil_score = silhouette_score(X_scaled_clust, labels)\n    silhouette_scores.append(sil_score)\n\n    kmodels[k] = {\n        \"model\": kmeans,\n        \"labels\": labels,\n        \"silhouette\": sil_score,\n        \"inertia\": inertia\n    }\n\n# Summary table for clustering\nclust_summary = pd.DataFrame({\n    \"K\": ks,\n    \"Average Silhouette\": np.round(silhouette_scores, 4),\n    \"Total Within-Cluster SS (Inertia)\": np.round(inertias, 2)\n})\n\nprint(\"\\nK-Means clustering summary:\")\ndisplay(clust_summary)\n\n# Elbow plot\nplt.figure(figsize=(6,4))\nplt.plot(ks, inertias, marker=\"o\")\nplt.xlabel(\"Number of clusters K\")\nplt.ylabel(\"Total within-cluster sum of squares (Inertia)\")\nplt.title(\"Elbow Method for K-Means\")\nplt.grid(True)\nplt.show()\n\n# Silhouette plot\nplt.figure(figsize=(6,4))\nplt.plot(ks, silhouette_scores, marker=\"o\")\nplt.xlabel(\"Number of clusters K\")\nplt.ylabel(\"Average Silhouette Coefficient\")\nplt.title(\"Silhouette Coefficient for different K\")\nplt.grid(True)\nplt.show()\n\n# Choose best K based on silhouette\nbest_k_index = int(np.argmax(silhouette_scores))\nbest_k = ks[best_k_index]\nprint(\"Best K based on silhouette score:\", best_k)\n\n# Simple 2D visualization using first two original features\nfeature_names = X_clust.columns\nfeat_x = feature_names[0]\nfeat_y = feature_names[1]\n\nfor k in ks:\n    labels = kmodels[k][\"labels\"]\n    plt.figure(figsize=(6,4))\n    plt.scatter(\n        X_clust[feat_x],\n        X_clust[feat_y],\n        c=labels,\n        s=10\n    )\n    plt.xlabel(feat_x)\n    plt.ylabel(feat_y)\n    plt.title(f\"K-Means Clusters (K={k}) using {feat_x} vs {feat_y}\")\n    plt.show()\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4a1bfd05",
      "cell_type": "markdown",
      "source": "## [6] Evaluation and Comparison\n\nIn this section, we summarize and compare the results of the data mining techniques.\n\n### 6.1 Classification\n\nThe table `clf_results_df` (printed above) summarizes the accuracy for each combination of:\n- Splitting criterion (Gini vs Entropy)\n- Train–test partition (90–10, 80–20, 70–30)\n\nFrom this table, we can identify:\n- The best-performing **criterion**.\n- The best **train–test split**.\n- The overall best model (also visualized by the final Decision Tree).\n\n### 6.2 Clustering\n\nThe table `clust_summary` contains, for each K:\n- Average silhouette coefficient\n- Total within-cluster sum of squares (inertia)\n\nUsing the silhouette and elbow plots, we can select the best K and interpret how the clusters separate water samples with different attribute values.",
      "metadata": {}
    },
    {
      "id": "3e4a334f",
      "cell_type": "code",
      "source": "# Re-display the main evaluation tables (optional)\n\nprint(\"=== Classification Accuracy Summary ===\")\ntry:\n    display(clf_results_df)\nexcept NameError:\n    print(\"clf_results_df is not defined. Please run the classification cell in Section 5.1.\")\n\nprint(\"\\n=== Clustering Summary ===\")\ntry:\n    display(clust_summary)\nexcept NameError:\n    print(\"clust_summary is not defined. Please run the clustering cell in Section 5.2.\")\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8d22d314",
      "cell_type": "markdown",
      "source": "## [7] Findings and Discussion\n\nIn this section, you should discuss the most important findings from both classification and clustering. For example:\n\n- **Classification**\n  - Which combination of criterion and train–test split achieved the best accuracy?\n  - Are there signs of overfitting (very high accuracy on a small test split)?\n  - Which attributes appear to be most important according to the Decision Tree structure?\n  - How do your results compare (even roughly) with the results reported in the selected     research paper that used a similar dataset or problem?\n\n- **Clustering**\n  - What is the best K according to the silhouette and elbow methods?\n  - How can you describe the clusters in terms of the water quality attributes.\n  - Do some clusters tend to contain more potable or more non-potable samples (even though     the label was not used during training)?\n\nFinally, summarize which technique (classification or clustering) is more useful for the stakeholders (e.g., water quality engineers) and what practical insights your project provides about water potability.\n",
      "metadata": {}
    },
    {
      "id": "542a6513",
      "cell_type": "markdown",
      "source": "## [8] References\n\nBelow is an example list of references in IEEE style. You should update it to match the exact research paper(s) you used in your project.\n\n1. A. Kadiwal, \"Water Potability,\" *Kaggle Datasets*, 2020. [Online]. Available: <https://www.kaggle.com/datasets/adityakadiwal/water-potability>.\n\n2. **[Replace with your main research paper]** Author(s), \"Title of the paper,\" *Conference or Journal Name*, vol. X, no. Y, pp. Z–AA, Year.\n\n3. F. Pedregosa *et al.*, \"Scikit-learn: Machine Learning in Python,\" *Journal of Machine Learning Research*, vol. 12, pp. 2825–2830, 2011.\n",
      "metadata": {}
    }
  ]
}